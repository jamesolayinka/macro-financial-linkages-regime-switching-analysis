{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "research_abstract",
            "metadata": {},
            "source": [
                "# Macro-Financial Linkages: A Quant-Research Perspective\n",
                "## Engineering Systematic Macro Features for Regime-Switching Models (2000â€“2025)\n",
                "\n",
                "---\n",
                "\n",
                "### Research Objective\n",
                "Traditional macroeconomic analysis often focuses on point-in-time event studies or static levels. From a quantitative finance perspective, however, the value of macroeconomic data lies in its **stochastic innovations (shocks)** and its role as a **regime filter** for asset returns. \n",
                "\n",
                "This research workflow transforms raw macroeconomic indicators into a professional feature set. We hypothesize that financial markets do not respond linearly to macro levels but rather systematically encode the **departure from trend** and the **interaction between states** (e.g., inflationary growth vs. deflationary recession).\n",
                "\n",
                "### Methodology Overview\n",
                "1. **Information Ingestion**: Centralizing disparate frequency data (monthly, quarterly) into a daily-aligned temporal framework.\n",
                "2. **The Macro State Layer**: Mapping slow-moving growth and inflation dynamics into a four-quadrant regime model.\n",
                "3. **Standardized Shocks**: Normalizing macro innovations by realized volatility to capture \"surprise\" magnitude.\n",
                "4. **Cross-Asset Sensitivity**: Projecting macro states onto equity and commodity risk premia."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "env_setup",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from scipy import stats\n",
                "import warnings\n",
                "\n",
                "# Professional Plotting Configuration\n",
                "plt.rcParams['figure.figsize'] = (12, 6)\n",
                "plt.rcParams['axes.titlesize'] = 14\n",
                "plt.style.use('ggplot')\n",
                "sns.set_theme(style=\"whitegrid\", palette=\"viridis\")\n",
                "warnings.filterwarnings(\"ignore\")\n",
                "\n",
                "print(\"Environment initialized: High-fidelity research mode active.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "section_1_header",
            "metadata": {},
            "source": [
                "## Section 1: Data Ingestion & Alignment\n",
                "\n",
                "Quantitative macro research requires a unified timeline. We integrate the preprocessed macro indicators with high-frequency asset prices, ensuring all information is correctly lagged to avoid look-ahead bias."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "data_loading",
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_macro_quant_data(base_dir=\"..\"):\n",
                "    \"\"\"\n",
                "    Loads macro and asset datasets, ensuring they are aligned on a daily business day index.\n",
                "    \"\"\"\n",
                "    # Load Macro\n",
                "    macro_path = os.path.join(base_dir, \"data/processed/macro_processed.csv\")\n",
                "    if not os.path.exists(macro_path):\n",
                "        macro_path = \"data/processed/macro_processed.csv\"\n",
                "    macro = pd.read_csv(macro_path, index_col=0, parse_dates=True)\n",
                "    \n",
                "    # Load Core Assets (SPY as Proxy for Equities, GLD for Commodities)\n",
                "    # We iterate through the raw directories to build a multi-asset panel\n",
                "    assets = {}\n",
                "    for folder in [\"equities\", \"commodities\", \"sectors\"]:\n",
                "        dir_path = os.path.join(base_dir, \"data/raw\", folder)\n",
                "        if not os.path.exists(dir_path):\n",
                "            dir_path = os.path.join(\"data/raw\", folder)\n",
                "            \n",
                "        for f in [x for x in os.listdir(dir_path) if x.endswith(\".csv\")]:\n",
                "            ticker = f.replace(\".csv\", \"\")\n",
                "            df = pd.read_csv(os.path.join(dir_path, f), index_col=0, parse_dates=True)\n",
                "            assets[ticker] = df.iloc[:, 0] # Assume first column is the target price\n",
                "            \n",
                "    prices = pd.DataFrame(assets).sort_index()\n",
                "    \n",
                "    # Align\n",
                "    common_idx = macro.index.intersection(prices.index)\n",
                "    return macro.loc[common_idx], prices.loc[common_idx]\n",
                "\n",
                "macro_raw, prices_raw = load_macro_quant_data()\n",
                "print(f\"Ingested {macro_raw.shape[1]} macro series and {prices_raw.shape[1]} asset tickers.\")\n",
                "print(f\"Time-span: {macro_raw.index.min().date()} to {macro_raw.index.max().date()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "section_2_header",
            "metadata": {},
            "source": [
                "## Section 2: The Macro State Layer (State Filter)\n",
                "\n",
                "### Economic Intuition\n",
                "Market participants do not price information in a vacuum. The propagation of a shock is conditional on the **Macro Regime**. We apply a Z-score transformation to define \"Extreme\" vs \"Normal\" states and construct a **Growth-Inflation Quadrant**.\n",
                "\n",
                "**Regime Definition:**\n",
                "- **Goldilocks**: High Growth, Low Inflation\n",
                "- **Stagflation**: Low Growth, High Inflation\n",
                "- **Reflation**: High Growth, High Inflation\n",
                "- **Deflationary Recession**: Low Growth, Low Inflation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "state_features",
            "metadata": {},
            "outputs": [],
            "source": [
                "def engineer_state_features(df):\n",
                "    \"\"\"\n",
                "    Transforms raw levels into standardized state indicators.\n",
                "    \"\"\"\n",
                "    states = pd.DataFrame(index=df.index)\n",
                "    \n",
                "    # 1. Growth Proxy (Industrial Production YoY)\n",
                "    states['growth_z'] = (df['INDPRO'].pct_change(252) - df['INDPRO'].pct_change(252).rolling(1260).mean()) / df['INDPRO'].pct_change(252).rolling(1260).std()\n",
                "    \n",
                "    # 2. Inflation Proxy (CPI YoY)\n",
                "    states['inflation_z'] = (df['CPIAUCSL'].pct_change(252) - df['CPIAUCSL'].pct_change(252).rolling(1260).mean()) / df['CPIAUCSL'].pct_change(252).rolling(1260).std()\n",
                "    \n",
                "    # 3. Monetary Tightness (Fed Funds vs 10Y Yield)\n",
                "    states['yield_curve'] = df['DGS10'] - df['FEDFUNDS']\n",
                "    \n",
                "    # 4. Sentiment (Michigan Consumer Sentiment standardized)\n",
                "    states['sentiment_z'] = (df['UMCSENT'] - df['UMCSENT'].rolling(1260).mean()) / df['UMCSENT'].rolling(1260).std()\n",
                "    \n",
                "    # Define Quadrants (Binary Filters)\n",
                "    states['is_high_growth'] = (states['growth_z'] > 0).astype(int)\n",
                "    states['is_high_inflation'] = (states['inflation_z'] > 0).astype(int)\n",
                "    \n",
                "    return states.dropna()\n",
                "\n",
                "macro_states = engineer_state_features(macro_raw)\n",
                "\n",
                "fig, ax = plt.subplots(1, 2, figsize=(16, 5))\n",
                "sns.lineplot(data=macro_states[['growth_z', 'inflation_z']], ax=ax[0])\n",
                "ax[0].set_title(\"Standardized Growth & Inflation States\")\n",
                "ax[0].axhline(0, color='black', linestyle='--')\n",
                "\n",
                "macro_states[['is_high_growth', 'is_high_inflation']].value_counts().plot(kind='bar', ax=ax[1])\n",
                "ax[1].set_title(\"Frequency of Macro Quadrants\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "section_3_header",
            "metadata": {},
            "source": [
                "## Section 3: Shocks & Information Catalysts\n",
                "\n",
                "### Statistical Rationale\n",
                "To build predictive signals, we must convert non-stationary macro levels into **stationary innovations**. We define a \"Macro Shock\" as the daily innovation normalized by its rolling standard deviation. This allows us to compare the relative impact of a VIX spike vs a Fed Funds move on the same scale."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "shock_features",
            "metadata": {},
            "outputs": [],
            "source": [
                "def engineer_shock_features(df):\n",
                "    \"\"\"\n",
                "    Constructs volatility-normalized shocks for market-moving variables.\n",
                "    \"\"\"\n",
                "    shocks = pd.DataFrame(index=df.index)\n",
                "    \n",
                "    # Interest Rate Shocks\n",
                "    shocks['rate_shock'] = df['DGS10'].diff() / df['DGS10'].diff().rolling(60).std()\n",
                "    \n",
                "    # Energy Shocks (Log returns normalized)\n",
                "    shocks['energy_shock'] = np.log(df['DCOILWTICO']).diff() / np.log(df['DCOILWTICO']).diff().rolling(60).std()\n",
                "    \n",
                "    # Volatility Shocks (VIX levels change normalized)\n",
                "    shocks['vol_shock'] = df['VIXCLS'].diff() / df['VIXCLS'].diff().rolling(60).std()\n",
                "    \n",
                "    return shocks.dropna()\n",
                "\n",
                "macro_shocks = engineer_shock_features(macro_raw)\n",
                "sns.histplot(macro_shocks['rate_shock'], kde=True, color='teal')\n",
                "plt.title(\"Distribution of Normalized Interest Rate Shocks\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "section_4_header",
            "metadata": {},
            "source": [
                "## Section 2: Macro-Financial Sensitivity (Dynamic Linkages)\n",
                "\n",
                "### The Research Question\n",
                "How does the linkage between equities (SPY) and commodities (GLD) shift across macro states? We use rolling correlations weighted by macro shocks to identify periods of **regime decoupling**."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "research_analytics",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate returns\n",
                "returns = np.log(prices_raw).diff().dropna()\n",
                "\n",
                "# Rolling Correlation: Equities vs Commodities\n",
                "rolling_corr = returns['SPY'].rolling(126).corr(returns['GLD'])\n",
                "\n",
                "plt.figure(figsize=(14, 6))\n",
                "plt.plot(rolling_corr, label='SPY-GLD 6M Rolling Correlation', color='darkblue')\n",
                "plt.fill_between(rolling_corr.index, 0, 1, where=(macro_states['is_high_inflation'] == 1), \n",
                "                 color='red', alpha=0.1, label='High Inflation Regime')\n",
                "plt.title(\"Asset Correlation Dynamics across Inflationary Regimes\")\n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "export_features_header",
            "metadata": {},
            "source": [
                "## Section 5: Integrated Feature Export\n",
                "\n",
                "Finally, we consolidate our engineered states and shocks into a single master feature file for subsequent Markov-Switching modeling."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "export_logic",
            "metadata": {},
            "outputs": [],
            "source": [
                "final_features = pd.concat([macro_states, macro_shocks], axis=1).dropna()\n",
                "output_path = \"../data/processed/integrated_features.csv\"\n",
                "if not os.path.exists(\"../data/processed\"):\n",
                "    output_path = \"data/processed/integrated_features.csv\"\n",
                "\n",
                "final_features.to_csv(output_path)\n",
                "print(f\"Success: {final_features.shape[0]} samples exported to {output_path}\")\n",
                "final_features.tail()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}